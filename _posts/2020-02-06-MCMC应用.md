---
title: 马尔科夫链蒙特卡洛应用初窥
categories:
- 统计
tags:
- 马尔科夫链
- 蒙特卡洛
- LDA
---

在本文中，笔者会持续更新一些应用实例来帮助理解MCMC，以及一些现有的工具包使得我们更快的上手。(我的这句中文为嘛看起来像机翻)
用采样的方法不单单可以帮我们找到比较好的参数，也可以帮我们发现这些参数的分布。

## 常用的工具
[**PyMC3**](https://docs.pymc.io)

## 一个简单的例子
根据贝叶斯公式:
$$p(\theta | x) = \frac{p(\theta)p(x|\theta)}{p(x)} \propto p(\theta)p(x|\theta)$$

其中$x$表示我们的观测数据, $p(\theta)$ 表示参数的先验分布，可以理解为在看到数据$x$之前参数的分布，$p(\theta \| x)$表示的是后验分布，可以理解为在看到数据之后，参数的分布，$p(x \|\theta)$是'likelihood'，表示在当前参数下，我们观测数据的概率。

*“A Bayesian is one who, vaguely expecting a horse and catching a glimpse of a donkey, strongly concludes he has seen a mule” (Senn)* :smile:

例子[1]:
体重和身高的表达式(一条样本):

$$weight_i = \beta_0 + \beta_1 height_i + e_i  \quad e_i \sim N(0, \sigma^2)$$

其中各个参数的分布如下:

$$\beta_0 \sim N(0, m_0), \beta_1 \sim N(0, m_1), \sigma^2 \sim \Gamma^{-1}(\epsilon, \epsilon) \quad m_0=m_1=10^6, \epsilon=10^{-3}$$


我们的采样目标概率密度分布是后验分布: $p(\beta_0, \beta_1, \sigma^2 \| y)$， 但是如果直接在这个分布上采样会有多维的问题，所以我们可以用吉布斯采样，把它变成条件后验分布，分别采样:

$$p(\beta_0|y,\beta_1, \sigma^2),p(\beta_1|y, \beta_0, \sigma^2), p(\sigma^2|y, \beta_0, \beta_1)$$

*吉布斯采样: 先初始化这几个参数，然后按照上面的条件后验分布分别为各个参数采样，将三个新参数加入到样本集，继续这个循环进行采样*

在之前的吉布斯采样中，我们知道因为转移函数也是目标概率密度函数$p$，所以我们希望我们的$p$能比较容易抽样。所以我们来分别看看这几个后验分布表达式(省略计算过程，详细请见[1]):

$$\begin{align}p(\beta_0|y,\beta_1, \sigma^2) &\propto p(\beta_0)p(y|\beta_0, \beta_1, \sigma^2)\\
m_0 &\rightarrow \infty, \beta_0 \sim N(\frac{1}{N}\sum_i (y_i-x_i\beta_1), \frac{\sigma^2}{N})\end{align}$$


$$\begin{align}p(\beta_1|y,\beta_0, \sigma^2) &\propto p(\beta_1)p(y|\beta_0, \beta_1, \sigma^2)\\
m_0 &\rightarrow \infty, \beta_1 \sim N(\frac{\sum_i x_iy_i-\beta_0\sum_ix_i}{\sum_ix_i^2}, \frac{\sigma^2}{\sum_ix_i^2})\end{align}$$


$$\begin{align}p(1/\sigma^2|y,\beta_0, \beta_1) &\propto p(1/\sigma^2)p(y|\beta_0, \beta_1, \sigma^2)\\
p(1/\sigma^2|y,\beta_0, \beta_1)  &\sim \Gamma(a,b) \\
&where \quad a=\epsilon +\frac{N}{2}, b=\epsilon + \frac{1}{2}\sum_ie_i^2
\end{align}$$

我们可以发现这三个条件后验分布都是比较容易采样的函数。因此只要带入到我们之前的吉布斯采样函数中即可。

### LDA
我们在这一节中来简单地看一下吉布斯采样如何求解潜在狄利克雷分配(latent Dirichlet allocation, LDA)，LDA的另一种求解方法是用变分EM算法，下篇文章我们来一探变分EM算法。下面我们先回顾几个知识点，一些证明会略掉，大家可以去李航老师的《统计学习方法》[2]中得到具体的证明过程。

*多项式分布* 若多元离散随机变量$X = (X_1, X_2, ..., X_k)$的概率质量函数为：

$$\begin{align} P(X_1=n_1, X_2=n_2, \dots, Xk=n_k) &= \frac{n!}{n_1!n_2!\dots n_k!}p_1^{n_1}p_2^{n_2}\dots p_k{n_k}\\
&=\frac{n!}{\prod_{i=1}^k n_i!} \prod_{i=1}^kp_i^{n_i}
 \end{align}$$

其中$ p=(p_1,p2,\dots, p_k), \quad p_i \ge 0, i=1,2, \dots, k, \quad \sum_{i=1}^k p_i=1, \sum_{i=1}^k n_i = n, $, 则称随机变量$X$服从参数为$(n,p)$的多项式分布，记作$X \sim Mult(n,p)$。当实验的次数$n=1$，多项式分布变为类别分布。

*狄利克雷分布* （Dirichlet distribution），在贝叶斯学习中，狄利克雷分布常常作为多项式分布使用(狄利克雷分布是多项式分布的共轭先验)。

$$\begin{align} 
p(\theta|\alpha) &= \frac{1}{B(\alpha)} \prod_{i=1}^k \theta_i^{\alpha_i-1} \\
B(\alpha) &= \frac{\prod_{i=1}^k \Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^k \alpha_i)}\\
B(\alpha)&=\int\prod_{i=1}^k \theta_i^{\alpha_i-1}d\theta 是规范化因子，称为多元贝塔函数（证略）
\end{align}$$

其中 $\sum_{i=1}^k \theta_i=1, \theta_i \ge0, \alpha=(\alpha_1, \alpha_2, \dots, \alpha_k), \alpha_i>0, i=1,2, \dots, k$, 则称随机变量$\theta$服从参数为$\alpha$的狄利克雷分布，记作 $\theta \sim Dir(\alpha)$。$\Gamma(s)$为伽马函数。





## References

[1] http://ukdataservice.ac.uk/media/307220/presentation4.pdf

[2]










