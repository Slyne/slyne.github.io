---
title: 变分推理
description: 简单浏览变分推理(Variational Inference, VI)的一些知识点 (持续更新)
categories:
- 统计
- 机器学习
tags:
- VI
- LDA
---

## 背景介绍
这篇是一个阅读VI相关的资料([1],[2],[3],[4]) 的一个阅读笔记，首先根据[1]简单的介绍基本概念，然后根据[2],[3],[4]进行内容的丰富和增加。希望每天都可以更新一丢丢，渐渐搞清楚这个概念。

我们假设我们的观测变量为 $x=x_{1:n}$, $z=z_{1:m}$ 是隐变量以及额外的一些参数$\alpha$。假设我们的模型是联合概率分布 $p(x,z)$，我们的后验概率分布为:

$$ p(z|x,\alpha) = \frac{p(z,x|\alpha)}{\int_z p(z,x|\alpha)} $$

后验概率将模型和数据结合了起来，它会用在很多下游的任务中。（计算后验也是变分推理能解决的一个实例）。但是，这里的分母很难计算，我们不难想象假设 $z$是离散变量，我们需要枚举所有的组合。举个例子，假设我们需要为一段$n$个字的文本进行标注，每个文字的标注为$z_i$，它的取值有 $K$ 个，那么这里我们的所有组合就有 $K^n$ 种。（虽然如果文字长度不长并且标注的取值很少的话还是可以穷举的）当 $n$ 变得很大的时候，这个问题就变得很难搞了(intractable)。如果不是离散的，很多模型对应的分母的积分都很难求解。因此，近似后验推理(approximate posterior inference) 就成了贝叶斯统计中一个很核心的问题。

它的基本思想是，为隐变量挑选一个分布族(a family of distributions)，它的参数叫做变分参数(variational parameters), $q(z_{1:m}\|v)$。接着我们需要找到一组参数使得隐变量概率分布 $q^*(z)$ 尽可能的接近我们需要求解的后验概率分布 $p(z\|x)$。然后用 $q$ 来代替后验概率分布进行一些其他任务。(通常真正的后验概率分布其实不在这个挑选的变分分布族里面。)

### Kullback-Leibler Divergence
KL Divergence 来自于信息论，它的定义如下:

$$ \begin{align} KL(q||p) &= E_q \left[\log\frac{q(z)}{p(z|x)} \right]  \quad(1) \\
\\
                                   &= E_q[\log{q(z)}] - E_q[\log p(z|x)] \\
\\
                                   &= E_q[\log q(z)] - E_q[\log p(x,z)] + \log p(x) \\
\\
                                   &= \log p(x) - \{E_q[\log p(x,z)] - E_q[\log q(z)] \} \quad(2)
\end{align}$$

接着，根据 Jensen 不等式，我们知道如果函数 $f$ 是凹的(concave)，那么它满足：$f(E[X]) \ge E[f(X)]$
这里，我们知道$\log$ 函数是凹的，所以有:

$$\begin{align} 
\log p(x) &= \log \int_z p(x, z) \\ \\
             &= \log \int_z p(x, z) \frac {q(z)}{q(z)} \\ \\
             &= \log \left( E_q \left[ \frac{p(x, z)}{q(z)} \right] \right) \\ \\
             &\ge E_q[ \log p(x,z)] - E_q[\log q(z)]

\end{align}$$

其中，$\log p(x)$ 被称为证据(evidence)，右端被称为证据下界(evidence lower bound, ELBO)。 因此如果想减小KL散度使得两个分部尽可能接近，我们就需要增大证据下界ELBO。


### Mean Field
平均场 (mean field)变分推理中，我们假设变分家族可以因式分解成：

$$ q(z_1, \dots, z_m) = \prod_{j=1}^m q(z_j)$$

即每个变量都是独立的(条件独立于参数 $v_j$ )。（但很多时候隐变量其实是互相依赖的）

接着我们利用这个因式分解来优化证据下界 ELBO，可以用一些优化方法比如坐标上升，迭代的优化每个变分分布(保持其它的不变)。

### 变分推理步骤







## References

[1] https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf

[2] https://arxiv.org/abs/1601.00670

[3] Christopher M Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics)[M]. Springer-Verlag New York, Inc. 2006.

[4] 李航. 统计学习方法第二版[J]. 2019.

