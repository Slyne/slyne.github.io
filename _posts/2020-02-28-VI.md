---
title: 变分推理
description: 简单浏览变分推理(Variational Inference, VI)的一些知识点 (持续更新)
categories:
- 统计
- 机器学习
tags:
- VI
- LDA
---

## 背景介绍
这篇是一个阅读VI相关的资料([1],[2],[3],[4]) 的一个阅读笔记，首先根据[1]简单的介绍基本概念，然后根据[2],[3],[4]进行内容的丰富和增加。希望每天都可以更新一丢丢，渐渐搞清楚这个概念。

我们假设我们的观测变量为 $x=x_{1:n}$, $z=z_{1:m}$ 是隐变量以及额外的一些参数$\alpha$。假设我们的模型是联合概率分布 $p(x,z)$，我们的后验概率分布为:

$$ p(z|x,\alpha) = \frac{p(z,x|\alpha)}{\int_z p(z,x|\alpha)} $$

后验概率将模型和数据结合了起来，它会用在很多下游的任务中。（计算后验也是变分推理能解决的一个实例）。但是，这里的分母很难计算，我们不难想象假设 $z$是离散变量，我们需要枚举所有的组合。举个例子，假设我们需要为一段$n$个字的文本进行标注，每个文字的标注为$z_i$，它的取值有 $K$ 个，那么这里我们的所有组合就有 $K^n$ 种。（虽然如果文字长度不长并且标注的取值很少的话还是可以穷举的）当 $n$ 变得很大的时候，这个问题就变得很难搞了(intractable)。如果不是离散的，很多模型对应的分母的积分都很难求解。因此，近似后验推理(approximate posterior inference) 就成了贝叶斯统计中一个很核心的问题。

它的基本思想是，为隐变量挑选一个分布族(a family of distributions)，它的参数叫做变分参数(variational parameters), $q(z_{1:m}\|v)$。接着我们需要找到一组参数使得隐变量概率分布 $q^*(z)$ 尽可能的接近我们需要求解的后验概率分布 $p(z\|x)$。然后用 $q$ 来代替后验概率分布进行一些其他任务。(通常真正的后验概率分布其实不在这个挑选的变分分布族里面。)

### Kullback-Leibler Divergence
KL Divergence 来自于信息论，它的定义如下:

$$ \begin{align} KL(q||p) &= E_q \left[\log\frac{q(z)}{p(z|x)} \right]  \quad(1) \\
\\
                                   &= E_q[\log{q(z)}] - E_q[\log p(z|x)] \\
\\
                                   &= E_q[\log q(z)] - E_q[\log p(x,z)] + \log p(x) \\
\\
                                   &= \log p(x) - \{E_q[\log p(x,z)] - E_q[\log q(z)] \} \quad(2)
\end{align}$$

接着，根据 Jensen 不等式，我们知道如果函数 $f$ 是凹的(concave)，那么它满足：$f(E[X]) \ge E[f(X)]$
这里，我们知道$\log$ 函数是凹的，所以有:

$$\begin{align} 
\log p(x) &= \log \int_z p(x, z) \\ \\
             &= \log \int_z p(x, z) \frac {q(z)}{q(z)} \\ \\
             &= \log \left( E_q \left[ \frac{p(x, z)}{q(z)} \right] \right) \\ \\
             &\ge E_q[ \log p(x,z)] - E_q[\log q(z)]

\end{align}$$

其中，$\log p(x)$ 被称为证据(evidence)，右端被称为证据下界(evidence lower bound, ELBO)。 因此如果想减小KL散度使得两个分部尽可能接近，我们就需要增大证据下界ELBO。


### Mean Field
平均场 (mean field)变分推理中，我们假设变分家族可以因式分解成：

$$ q(z_1, \dots, z_m) = \prod_{j=1}^m q(z_j)$$

即每个变量都是独立的(条件独立于参数 $v_j$ )。（但很多时候隐变量其实是互相依赖的）

接着我们利用这个因式分解来优化证据下界 ELBO，可以用一些优化方法比如坐标上升，迭代的优化每个变分分布(保持其它的不变)。

### 变分推理步骤
在我们选择了我们的分布族之后，我们将其平均场的表达式带入到证据下界中:

$$\begin{align} 
\mathcal{L}(q) &= \int \prod_i q_i \left \{ \ln p(X,Z) - \sum_i \ln q_i \right \} \mathrm{dZ} \\ \\
&=    \int q_j \left \{ \int \ln p(X,Z) \prod_{i \neq j} q_i \mathrm{dZ_i} \right \} \mathrm{dZ_j} - \int q_j \ln q_j \mathrm{dZ_j} + const \\
&= \int q_j \ln \widetilde{p}(X,Z_j) \mathrm{dZ_j} - \int q_j \ln q_j \mathrm{dZ_j} + const              
\end{align}$$

其中,

$$
\ln \widetilde{p}(X,Z_j) \mathrm{dZ_j} =  \mathbb{E}_{i \neq j} \left[ \ln p(X,Z) \right] + const
$$

这里的 $\mathbb{E}_{i \neq j} \left[ \dots \right] $ 表示的对于所有的变量 $z_i (i \neq j)$ 在变分分布 $q$ 上的期望。

假设我们让 $\{ q_{i \neq j}\}$ 保持不变，并且我们需要最大化证据下界 $\mathcal{L}(q)$。不难看出 $\mathcal{L}(q)$ 其实就是一个负的在 $q_j{Z_j}$ 和 $\widetilde{p}(X,Z_j)$之间的KL散度，因此最大化下界就变成了最小化KL散度，而这个最小的情况显然发生在$q_j{Z_j} = \widetilde{p}(X,Z_j)$，这两个分布相同的情况下。（其实我们也可以直接对 $\Z_j$求导）这样我们就获得了最优解 $$q^*_j(Z_j)$的表达式为:

$$\ln q^*_j(Z_j) = \mathbb{E}_{i \neq j} [\ln p(X,Z)] + const$$

这个式子表明因子 $\ln q_j $的最优解只需要考虑在所有隐变量和观测变量上考虑他们的联合分布，接着根据其它因子 ${q_i} (i \neq j)$对这个联合分布取期望。这里的常量可以理解为是正则项，归一化 $q^*_j(Z_j)$，对等号两边取exp，得到:

$$q^*_j(Z_j) = \frac{\mathbb{E}_{i \neq j}[\ln p(X,Z)]}{\int \exp(\mathbb{E}_{i \neq j} [\ln p(X,Z)]) \mathrm{dZ_j}}$$

在实际应用中，我们会初始化好所有的 $q_i(Z_i)$ 然后迭代更新每个因子。（收敛是可以保证的，因为下界关于每个因子是凸(convex)的, 注意是局部最优解）。另外，也有把上面的表达式写成这个样子的:

$$ q^*(z_j) \propto \exp{ E_{-k}[\log p(z_j, Z_j, x)]} $$



### 举个例子




## References

[1] https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf

[2] https://arxiv.org/abs/1601.00670

[3] Christopher M Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics)[M]. Springer-Verlag New York, Inc. 2006.

[4] 李航. 统计学习方法第二版[J]. 2019.

